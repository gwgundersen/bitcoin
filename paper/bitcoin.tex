\documentclass{article} % For LaTeX2e
\usepackage{cos424,times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{subcaption}

\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true
}

\title{Predicting transactions in Bitcoin blockchain data}

\author{
Gregory Gundersen\\
Princeton University\\
\texttt{ggundersen@princeton.edu}
\And
Prakhar Kumar \\
Princeton University \\
\texttt{prakhark@princeton.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We predict future Bitcoin transactions for pairs of addresses based on historical transaction data. The training dataset consists of transaction counts between sender-receiver address pairs that interacted within a one-year period. The test data consists of address pairs which did not interact within that one year period but may have in the future. We convert the training dataset into an extremely sparse binary adjacency matrix of all possible combinations of addresses and then use matrix completion methods to reconstruct a low-rank approximation of the data which can be used for prediction. We find that two matrix completion methods, singular value decomposition and nonnegative matrix factorization, do better than random guessing.
\end{abstract}

\section{Introduction}

\subsection{Background}

Bitcoin is an online virtual currency introduced in 2008 by Satoshi Nakamoto \cite{nakamoto2008bitcoin}. All transactions are cryptographically signed and recorded on a public ledger called the blockchain. The blockchain allows the global, peer-to-peer network of Bitcoin users to validate all transactions and thereby eschew a centralized bank. See \cite{nielsen2013bitcoin} for a detailed explanation of how Bitcoin works. The upshot is that with Bitcoin, everyone is the bank and the blockchain is an account of the entire history of all anonymized transactions.

In this paper, we predict transactions between pairs of addresses in a portion of the Bitcoin blockchain. The training dataset consists of transaction counts between sender-receiver address pairs that did interact, i.e. there are no zero counts, within a one year period. Therefore the entire training dataset consists of positive examples. The testing dataset consists of some pairs of address which did not interact and some which did in a subsequent year. We find that matrix completion methods to estimate the probability of transactions between a sender-receiver pair work better than traditional classification models. In particular, we investigate two matrix completion methods, singular value decomposition (SVD) and nonnegative matrix factorization (NMF). Their performances were evaluated on accuracy, precision, recall, and area under the curve (AUC). We found that NMF generalizes better on the test data in comparison to SVD.

In addition, we explored an SVM-based classifier on a modified dataset in which the additional features are the in-degree and out-degree of both the receiver and sender. However, we found that the test accuracy was always around 90\% with low accuracy and recall despite constructing a balanced training set consisting of equal number of positive and negative labels.

\subsection{Related work}

Matrix completion methods are frequently used for recommender systems \cite{koren2009matrix}. This is because recommender systems typically rely with incomplete and/or sparse data to provide suitable recommendations to the users for services like Amazon, Facebook, and Netflix. The sparsity is common because in most scenarios a user cannot rate or even be aware of most items. The central challenge, then, is to develop methods to fill in missing recommendations based on the user and others' recommendations. The key assumption is that user preferences are generally guided by a small number of underlying factors and so decomposing the data into a low-rank representation creates a more general model.

This approach can be used for this work predicting Bitcoin transactions because we assume the number of factors which determine whether or not two users will interact is much smaller than the number of addresses in the dataset. The significant sparsity of our dataset makes it ideal to implement matrix factorization methods for identifying low-rank decompositions of the original data.

The Netflix Prize is a popular example of recommender systems employing large scale SVD and other matrix completion methods to estimate the probable preferences of users for movies \cite{bennett2007netflix}.

\section{Methods}

\subsection{Data}

The training dataset consists of transactions between 3,348,026 sender-receiver pairs and the number of times the sender paid the receiver. The transactions were made between March 2012 to March 2013. The testing dataset consists of sender-receiver pairs which did not interact in the training set time period but may have by the following year, May 2014. The full dataset contains 444,075 unique addresses, and therefore over 19.7 billion possible unordered pairs of addresses. A binary adjacency matrix is extremely sparse, with only $1.7 \times 10^{-5}$ nonzero values.

\subsection{Classifiers}\label{classifiers}

\paragraph{Spotlight: Nonnegative matrix factorization}

Nonnegative matrix factorization (NMF) is a method for approximating a matrix as the outer product of two other, typically smaller, matrices. Formally, a matrix $X$ is factorized into matrices $V$ and $U$ such that:

\begin{equation} \label{eq:1}
X \approx V \times U
\end{equation}

Where $X$ is an $n \times m$ matrix, $V$ is an $n \times k$ matrix, and $U$ is a $k \times m$ matrix. The value $k$ is a hyperparameter of the model which indicate the number of latent variables that approximate $V$ (Figure \ref{fig:matrix_completion}).

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.2]{figures/matrix_completion}
    \caption{\small Matrix $X$ is factorized into matrices $V$ and $U$.}
    \label{fig:matrix_completion}
\end{figure}

The key assumption is that the approximated matrix has low-rank. In other words, not every cell in the matrix is a free variable. Instead, each cell value is the dot product of two $k$-dimensional vectors. The value of the low-rank assumption is that it reduces the number of free parameters of a model. In the most extreme case, $k=1$, a model that would typically have $n \times m$ parameters can be approximated by just $n + m$ parameters. In general, the number of parameters is $k \times (n + m)$.

Prediction for matrix completion methods such as NMF are as follows. First, we approximate the observed data $X$ using matrix factorization (Equation \ref{eq:1}). We then complete the matrix by multiplying the two matrix factors together:

$$
\hat{X} = V \times U
$$

In principle, $\hat{X}$ is a matrix that captures the latent structure in $X$. We can then base predictions on the value in the completed matrix. For example, since our matrix $X$ contains sender addresses as rows and receiver addresses as columns (or vice versa), then the prediction for whether or not the $i$th address made a transaction with the $j$th address would be found in $\hat{X}[i, j]$.

There are many algorithms for solving matrix factorization for NMF. See \cite{lee2001algorithms} for details. One straightforward way is to use gradient descent to minimize the difference between the observed data and the factorized estimates. Formally, given observation $X_{ij}$, we want to find low-dimensional vectors $v$ and $u$ such that $X_{ij} = v_i \cdot  u_j$ by minimizing the following objective function:

$$
f(u, v) = \sum_{i,j \text{ observed}} (X_{ij} - v_i \cdot u_j)^2
$$

Ultimately, the learned matrices $V$ and $U$ are uninterpretable. But like many machine learning methods, we assume that if we can approximate the data well with fewer numbers, we might capture generalities in the data. Other dimensionality reduction methods such as SVD and PCA rely on this thinking as well.

\subsection{Classification methods.} We employed several matrix completion methods to find low-rank approximations of the data. We also tried an SVM classifier with random subsampling from a full dataset of all possible transactions.

\begin{itemize}

\item \textbf{SVM.} TK TK TK.

\item \textbf{SVD.} SVD is a matrix factorization and dimensionality reduction technique. It is generally useful when we need to determine a low-rank approximation of a large and sparse matrix.     

\item \textbf{NMF.} Non-negative matrix factorization is another well-known matrix completion technique. It is especially useful for decomposition of multivariate data into product of two non-negative matrices. By restricting the rank of the matrices we can create a dense low rank approximation of the data and extract the underlying meaningful structures in the data.

\end{itemize}

\section{Results}

The results for the performance of SVD and NMF were evaluated on various metrics such as accuracy score, precision, recall and F-1 score. The results have been summarized in Table [TK table]. Moreover, ROC curves for both the methods were analyzed to compare the performances for varying classification thresholds [TK table] . It can be clearly observed from the two ROC curves that NMF outperforms the SVD since the performance of SVD based approach falls below the random guess at a certain threshold.

\paragraph{SVM and Logistic regression.} TK TK.

\paragraph{NMF.} TK TK.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/singular_values_log_yscale}
    \caption{\small TK TK}
    \label{fig:svd}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/roc}
    \caption{\small TK TK}
    \label{fig:roc}
\end{subfigure}
\caption{\small TK TK}
\label{fig:test}
\end{figure}

\paragraph{SVD.} In order to decompose the large sparse data matrix as a product of two low-rank matrices, we analyzed the singular values by plotting these singular values for different components. We decided to select the top 20 components by observing a ``knee'' in the plot (Figure \ref{fig:svd}). Thus, the assumption was that the basic factors determining whether a transaction would take place can be explained by these 20 components.

\section{Discussion}

This work has attempted at identifying the possibility of future transactions between previously unrelated addresses. Matrix completion methods have been used as the tools for doing the same.

\subsubsection*{Acknowledgments}

We benefited from class notes, precept discussion, and office hours for Princeton's COS 424.

\bibliography{ref}
\bibliographystyle{plain}

\end{document}
